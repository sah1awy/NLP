{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e14a6fe8-c122-4725-b603-5ade09743445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from string import punctuation\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4c6d5b6-3956-4fed-a52f-a9e445cc9adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167674"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"wonderland.txt\",'w',encoding='utf-8').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "592ca89b-eb6f-41de-910a-064be581f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "batch_sz = 128\n",
    "epochs = 30\n",
    "f_path = 'wonderland.txt'\n",
    "basename = os.path.basename(f_path)\n",
    "txt = open(f_path,encoding='utf-8').read()\n",
    "txt = txt.lower()\n",
    "txt = txt.translate(str.maketrans(\"\",\"\",punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd908608-ab7c-4d20-b2e2-26a8db50a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_remove = \"ù—‘’“”•™﻿\"\n",
    "txt = txt.translate(str.maketrans(\"\", \"\", chars_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f491e3ab-818c-4116-855b-4b972b850343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " 0123456789abcdefghijklmnopqrstuvwxyz\n",
      "Number of Characters: 158608\n",
      "Number of unique Characters 38\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(txt)\n",
    "vocab = ''.join(sorted(set(txt)))\n",
    "n_uq_chars = len(vocab)\n",
    "print(\"Vocabulary:\",vocab)\n",
    "print(\"Number of Characters:\",n_chars)\n",
    "print(\"Number of unique Characters\",n_uq_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06d6d5a-c25e-4aa1-8dcb-c35b38543813",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = {c:i for i,c in enumerate(vocab)}\n",
    "int2char = {i:c for i,c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811e9988-9465-4082-ab46-02b6042bbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(char2int,open(f\"{basename}-char2int.pickle\",'wb'))\n",
    "pickle.dump(int2char,open(f\"{basename}-int2char.pickle\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85831e47-8e9b-452e-8e5e-b1de8ba7eb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 19, 16, 1, 27]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_txt = [char2int[x] for x in txt]\n",
    "encoded_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d1a132-c92b-4d97-a22d-8d7e464ae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_txt = np.array(encoded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "332eedfd-e59f-4125-b855-3b6de8a29a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_data = tf.data.Dataset.from_tensor_slices(encoded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9057e13f-de3b-4a3e-a401-606e057cb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 t\n",
      "19 h\n",
      "16 e\n",
      "1  \n",
      "27 p\n",
      "29 r\n",
      "26 o\n",
      "21 j\n"
     ]
    }
   ],
   "source": [
    "for s in char_data.take(8):\n",
    "    print(s.numpy(),int2char[s.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55e5d115-920e-4912-9843-d8a83e54b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of alices adventures in wonderland\n",
      "\n",
      "    \n",
      "\n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "\n",
      "most other parts of the world at no cost and with almost no \n",
      "201\n",
      "--------------------------------------------------\n",
      "restrictions\n",
      "\n",
      "whatsoever you may copy it give it away or reuse it under the terms\n",
      "\n",
      "of the project gutenberg license included with this ebook or online\n",
      "\n",
      "at wwwgutenbergorg if you are not located in the \n",
      "201\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "seq = char_data.batch(2*seq_length+1,drop_remainder=True)\n",
    "for s in seq.take(2):\n",
    "    print(''.join([int2char[i] for i in s.numpy()]))\n",
    "    print(len(''.join([int2char[i] for i in s.numpy()])))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f3f3b02-c4f4-46be-8a7a-8e7cdf579327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:seq_length], sample[seq_length]))\n",
    "    for i in range(1, len(sample) - seq_length):\n",
    "        input_ = sample[i:i+seq_length]\n",
    "        target = sample[i+seq_length]\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "dataset = seq.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e359a021-ebe7-4717-b3d0-947cf4520541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    return tf.one_hot(input_, n_uq_chars), tf.one_hot(target, n_uq_chars)\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b73dce8-764a-4885-b895-f89aa861d56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(100, 38), dtype=tf.float32, name=None), TensorSpec(shape=(38,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2cf58786-e659-40bc-83f6-c8c54abe1143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: the project gutenberg ebook of alices adventures in wonderland\n",
      "\n",
      "    \n",
      "\n",
      "this ebook is for the use of a\n",
      "Target: n\n",
      "Input shape: (100, 38)\n",
      "Target shape: (38,)\n",
      "================================================== \n",
      "\n",
      "Input: he project gutenberg ebook of alices adventures in wonderland\n",
      "\n",
      "    \n",
      "\n",
      "this ebook is for the use of an\n",
      "Target: y\n",
      "Input shape: (100, 38)\n",
      "Target shape: (38,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 2 samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ceab8573-d11e-41da-8a74-b43a1d172a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.repeat().shuffle(1024).batch(batch_sz,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e988b2b-8a81-4c24-ae61-6ad79b4c29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(seq_length, n_uq_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_uq_chars, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bf52a23-e0b6-4cd0-b801-a6f104f351bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238/1238 [==============================] - 1297s 1s/step - loss: 2.2146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2237e666100>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_txt) - seq_length) // batch_sz, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c7fa22a-dc3b-4f96-98ed-6bd9fef9b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As We can see below we need more data, epochs, computation power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26a6f729-bb23-48ce-a258-cb71e838aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|███████████████████████████████████████████████████████████████| 400/400 [00:59<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: chapter xiii\n",
      "Generated text:\n",
      "cation and the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of the project of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_sz = len(char2int)\n",
    "seed = \"chapter xiii\"\n",
    "s = seed\n",
    "generated = \"\"\n",
    "n = 400\n",
    "for i in tqdm.tqdm(range(n), \"Generating text\"):\n",
    "    X = np.zeros((1, seq_length, vocab_sz))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (seq_length - len(seed)) + t, char2int[char]] = 1\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    next_index = np.argmax(predicted)\n",
    "    next_char = int2char[next_index]\n",
    "    generated += next_char\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", s)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "715b27f7-5604-4db6-bf72-ddc996d02da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
